\documentclass{beamer}
%\documentclass[handout]{beamer}
% This file is a solution template for:

% - Giving a talk on some subject.
% - The talk is between 15min and 45min long.
% - Style is ornate.

% Copyright 2004 by Till Tantau <tantau@users.sourceforge.net>.
%
% In principle, this file can be redistributed and/or modified under
% the terms of the GNU Public License, version 2.
%
% However, this file is supposed to be a template to be modified
% for your own needs. For this reason, if you use this file as a
% template and not specifically distribute it as part of a another
% package/program, I grant the extra permission to freely copy and
% modify this file as you see fit and even to delete this copyright
% notice. 


\mode<presentation>
{
  \usetheme{Montpellier}

  %\setbeamercovered{transparent}
  % or whatever (possibly just delete it)
}

\usepackage{xmpmulti} % package that defines \multiinclude

\usepackage[english]{babel}

\usepackage[latin1]{inputenc}

\usepackage{times}
\usepackage[T1]{fontenc}
% Or whatever. Note that the encoding and the font should match. If T1
% does not look nice, try deleting the line with the fontenc.

%\title [short title] (optional, use only with long paper titles)
\title{The Context Algorithm}

\author[Freund] % (optional, use only with lots of authors)
{Yoav Freund}
% - Give the names in the same order as the appear in the paper.
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute[Universities of Somewhere and Elsewhere] % (optional, but mostly needed)

\subject{Machine Learning}
% This is only inserted into the PDF information catalog. Can be left
% out. 

% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}



% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
%% \AtBeginSubsection[]
%% {
%%   \begin{frame}<beamer>
%%     \frametitle{Outline}
%%     \tableofcontents[currentsection,currentsubsection]
%%   \end{frame}
%% }


% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 

\beamerdefaultoverlayspecification{<+->}

\input{../macros}

\begin{document}

%\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
  \titlepage
\begin{itemize}
\item  Willems, Frans MJ, Yuri M. Shtarkov, and Tjalling J. Tjalkens. "The context-tree weighting method: Basic properties." (1995)  \\
\item  Willems, Frans MJ, Ali Nowbakht, and Paul AJ Volf. "Maximum a posteriori probability tree models." (2002)
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Outline}
  \tableofcontents[pausesections]
  % You might wish to add the option [pausesections]
\end{frame}

\section{Review}

\begin{frame}
\frametitle{The online Bayes Algorithm}
\begin{itemize}
\item {\color{blue} Total loss} of expert \R{$i$}
\R{$$L_i^t = - \sum_{s=1}^{t} \log p_i^s(c^s);\;\;\; L_i^0 = 0$$}
\item {\color{blue}Weight} of expert \R{$i$}
\R{$$\wt{t}{i} = \wt{1}{i} e^{-L_i^{t-1}} = \wt{1}{i} \prod_{s=1}^{t-1} p_i^s(c^s)$$}
\item
Freedom to choose initial weights.\\
 \R{$\wt{1}{t} \geq 0$}, \R{$\sum_{i=1}^n \wt{1}{i} = 1$}
\item {\color{blue}Prediction} of algorithm \R{$A$}
\R{\[
\vp_A^t = \frac{\sum_{i=1}^N \wt{t}{i} \vp_i^t}{\sum_{i=1}^N \wt{t}{i}}
\]}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Cumulative loss vs. Final total weight}

\onslide<1-> Total weight: \R{$W^t \doteq \sum_{i=1}^N \wt{t}{i}$}

\onslide<2-> \R{$$
\frac{W^{t+1}}{W^t}  =  \frac{\sum_{i=1}^N \wt{t}{i} e^{\log p_i^t(c^t)}}{\sum_{i=1}^N \wt{t}{i}} 
\onslide<3->          =   \frac{\sum_{i=1}^N \wt{t}{i} p_i^t(c^t)}{\sum_{i=1}^N \wt{t}{i}} 
\onslide<4->        =  p_A^t(c^t)
$$}
\onslide<5-> \R{$$ -\log \frac{W^{t+1}}{W^t} = -\log p_A^t(c^t) $$}
\R{\[
\onslide<8-> -\log W^{T+1} =
\onslide<6-> -\log \frac{W^{T+1}}{W^1} = -\sum_{t=1}^T \log p_A^t(c^t)
\onslide<7-> = L_A^T
\]}
\onslide<9-> \R{\bf EQUALITY} not bound!
\end{frame}

\begin{frame}
\frametitle{Simple Bound}
\begin{itemize}
\item Use non-uniform initial weights \R{$\sum_i \wt{1}{i} = 1$}
\item Total Weight is at least the weight of the best expert.
\R{\begin{eqnarray*}
L_A^T & = & -\log W^{T+1} 
\pause = -\log \sum_{i=1}^N \wt{T+1}{i} \\
\pause & = & -\log \sum_{i=1}^N \wt{1}{i} e^{-L_i^T} 
\pause \leq  - \log \max_i \paren{ \wt{1}{i} e^{-L_i^T} } \\
\pause & = & \min_i \paren{ L_i^T -\log \wt{1}{i} }
\end{eqnarray*}}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Universal Online coding}
  \includegraphics[width=3in]{figures/ArithmeticCoding_key.png}
\end{frame}

\begin{frame}
\frametitle{Combining large predictor families}
  \begin{itemize}
\item
  Log loss is {\bf mixable} = each predictor in the family can use a
  Bayesian combination of a family of sub-predictors, with no
  additional loss.
\item We talked about the KT preictor.
\item Today we consider the much richer set of variable length
  markov models.
\item The set of predictors is of exponential size, but the
  algorithm is efficient.
\end{itemize}
\end{frame}

\section{Fixed Length Markov Models}

%An Example
\begin{frame}
\frametitle{A fixed length Markov Model}
\begin{columns}
\begin{column}[t]{8cm}
\begin{itemize}
\item Observe a binary sequence.
\item \R{$x_1,\ldots,x_{t-1}$}
\item Predict next bit from past
\item \R{$P(x_t=1 | x_{t-1},x_{t-2},\ldots,x_1)$}
\item Use only last \R{$k$} bits
\item \R{$P(x_t=1 | x_{t-1},\ldots,x_{t-k})$}
\item Markov model of order \R{$k$}
\end{itemize}
\end{column}
\begin{column}[T]{5cm}
\includegraphics[width=4.5cm]{figures/PredictionTree.pdf}
\end{column}
\end{columns}
\end{frame}

%Using the KT predictor in each element
\begin{frame}
\frametitle{Learning a markov distribution}
\begin{itemize}
\item Each tree leaf is associated with a binary sequence
\R{$y_1,\ldots,y_k$} 
\item For each leaf keep two counters:
\begin{itemize}
\item \R{$a_{y_1,\ldots,y_k}$} = number of times \R{$x_{t-1}=y_1, \ldots, x_{t-k}=y_k$} \\ and \R{$x_t=0$}
\item \R{$b_{y_1,\ldots,y_k}$} = number of times \R{$x_{t-1}=y_1, \ldots, x_{t-k}=y_k$} \\ and \R{$x_t=1$}
\end{itemize}
\item Prediction (using Kritchevski Trofimov)
\R{\[p(x_t=1 | x_{t-1}=y_1,\ldots,x_{t-k}=y_k) 
= \frac{b_{y_1,\ldots,y_k}+1/2}{a_{y_1,\ldots,y_k}+b_{y_1,\ldots,y_k}+1}\]}
\item Total regret is at most \R{$2^{k-1} \log T$}
\end{itemize}
\end{frame}

\section{Variable Length Markov Model (VMM)}

%Why variable length is better than fixed length. (QU example)

\begin{frame}
\frametitle{How variable length markov can reduce regret}
\begin{columns}
\begin{column}[T]{5cm}
\multiinclude[graphics={width=4.5cm},format=pdf]{figures/PredictionTree}
\end{column}
\begin{column}[t]{8cm}
\begin{itemize}
\item Reducing number of leaves from \R{$8$} to \R{$4$} means 
\item reducing regret from \R{$4 \log T$} to \R{$2 \log T$}
\item English example: \\
\B{B \pause A \pause R \pause O \pause Q }\pause \R{U} \pause \B{E}
\item When we have little data, we can get better prediction even if the children are not 
\B{Exactly the same}
\end{itemize}
\end{column}
\end{columns}
\end{frame}

%defining complete trees
\begin{frame}
\frametitle{Prefix trees / Tries}
\begin{itemize}
\item In a prefix binary tree each node has either \R{0} or \R{2} children.
%\item A node with \R{1} child means that some past histories are not
%  covered.
\item A variable length markov model corresponds to a \B{prefix tree}.
\item You can think of a prefix trees as different \B{prunings} of a
  maximal tree.
\item We don't know a-priori which pruning to use!
\item The number of prunings trees increases exponentially with the
  number of nodes in the maximal tree.
\item We will use the Online Bayes to predict almost as well as the
  best prefix tree in hind-sight.
\item First - simple but inefficient algorithm, Second - efficient algorithms.
\end{itemize}
\end{frame}

% \section{Assigning weights to trees}

% Using bayesian method to perform almost as well as the best VMM
\section{Universal coding, an inefficient solution}

\begin{frame}
\frametitle{Using online Bayes to learn the structure}
\begin{itemize}
\item We assign to each tree an initial weight of \R{$2^{-n}$} where
  \R{$n$} is the number of \B{nodes} in the  pruned tree.
\item We combine the predictions of the trees using online Bayes.
\item The total regret would be \R{$\frac{l}{2}\log T + n$} where \R{$l$} is the number of \B{leaves} in the prefix tree.
%\item The papers do things slightly differently because they bound the depth of the tree by \R{$k$}.
\item This algorithm maintains a weight for each prefix tree.
\item The number of prunings of a full tree of depth $k$ is
  $O(2^{2^k})$ while maintaining all of the counts requires $O(2^k)$. 
\end{itemize}
\end{frame}

% Defining the distribution using a random process.

\section{Efficient Implementation}

\begin{frame}
\frametitle{Efficient implementation}
\begin{itemize}
\item \B{First idea:} Estimate probabilities of complete sequences and use conditional to generate predictions.
\item The prior weights are used for averaging the complete sequence probabilities - they don't need to be updated.
\item \B{Second idea:} Compute the average over the prior efficiently.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Efficient generation of prior}
\begin{itemize}
\item Prior distribution is generated by a stochastic recursion.
\item Start with root node (always exists)
\item For each node flip a fair coin.
\begin{itemize}
\item \B{Heads} Set node to be a leaf (\R{0} children)
\item \B{Tails} Create \R{2} children nodes to the node.
\end{itemize}
\item Defines a distribution over all prefix trees.
\item Probability of a tree with \R{$n$} nodes is \R{$2^{-n}$}
\end{itemize}
\end{frame}

% computing the posterior takes linear time because we can use the
% random process definition.`
\begin{frame}
\frametitle{Efficient averaging over the prior (observations)}
\begin{itemize}
\item Maintain a KT estimator at each node of the tree.
\item Allocate counters only for nodes that have been visited.
\item At iteration \R{$t$} only \R{$t$} counters need to be updated.
\item Only \R{$k$} counters if depth of tree is bounded.
\item Each node is visited on a subset of the iterations.
\item Subset corresponding to node is contained in subset corresponding to node's parent.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Efficient averaging over the prior (procedure)}
\begin{itemize}
\item This is not the method used in the original paper, it appears in\\
  ~\\
  Willems, Frans MJ, Ali Nowbakht, and Paul AJ Volf. "Maximum a posteriori probability tree models." (2002)

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Definitions}
\begin{itemize}
\item \R{$s$} is the past bit sequence corresponding to a node in the
  tree. The chidren of this node are \R{$0s$} and \R{$1s$}.
\item The sequence of past realized bits up to time \R{$t$} is denoted
  \R{$x_1^{t-1}$}, the \R{$t$}'th bit (RV) is denoted \R{$X_t$}
\item \R{$s$} determines a subsequence of \R{$x_1^{t-1}$}: the locations
  preceded by the reverse of \R{$s$}.
\item \R{$P_{\{e,w\}}^s(x_1^{t-1})$} is the probability assigned to the subsequence of \R{$x_1^{t-1}$} associated with the node $s$
\item \R{
    \[P_{\{e,w\}}^s(X_t=1|x_1^{t-1}) = \
      frac{P_{\{e,w\}}^s(x_1^{t-1},X_t=1)}{P_{\{e,w\}}^s(x_1^{t-1})}
      \]}
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The KT predictor}
  \begin{itemize}
    \item \R{$a_s(x_{1}^{t-1}), b_s(x_{1}^{t-1})$} count the number of 0's
  and 1's in the subsequence corresponding to \R{$s$}
\item     The KT estimate associated with node \R{$s$}.
  \R{$$
    P_e^s\paren{X_t=1 | x_1^{t-1}} = \frac{b_s(x_1^{t-1})+1/2}{a_s(x_1^{t-1})+b_s(x_1^{t-1})+1}
          $$}
\end{itemize}

\end{frame}

\begin{frame}
  \frametitle{The averaged predictor}
  \begin{itemize}
\item 
  \R{$P_w^s\paren{X_t=1 | x_1^{t-1}}$} is the conditional probability associated with the tree rooted at \R{$s$}
\item \R{
    \begin{eqnarray*}      P_w^s(x_1^{t-1},X_1=1) &=& \frac{1}{2}P_e^s(x_1^{t-1},X_t=1)\\
    &+& \frac{1}{2}P_w^{0s}(x_1^{t-1},X_t=1)P_w^{1s}(x_1^{t-1},X_t=1)
    \end{eqnarray*}
  }
\end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Conditioning and defining the mixing factor}
  \begin{small}
  \begin{itemize}
  \item \R{
      \begin{eqnarray*}
        &&P_w^s(X_t=1 | x_1^{t-1})\\
        &=& \frac{P_e^s(x_1^{t-1},X_t=1)
            +P_w^{0s}(x_1^{t-1},X_T=1)P_w^{1s}(x_1^{t-1},X_T=1)}{P_e^s(x_1^{t-1})+P_w^{0s}(x_1^{t-1})P_w^{1s}(x_1^{t-1})}\\
        &=& \frac{\beta^s(x_1^{t-1}) P^s_e(X_t=1|x_1^{t-1})
        +P_w^{0s}(X_t=1|x_1^{t-1})P_w^{1s}(X_t=1|x_1^{t-1})}
        {\beta^s(x_1^{t-1}) +1}
    \end{eqnarray*}
  }
\item Where \R{
    \[\beta^s(x_1^{t-1}) \doteq \frac{P^s_e(x_1^{t-1})}
  {P^{0s}_w(x_1^{t-1})P^{1s}_w(x_1^{t-1})}\]}
\end{itemize}
\end{small}

\end{frame}

\begin{frame}
\frametitle{Mixing Factors}
\begin{itemize}
\item The mixing factor for node \R{$s$} is\R{
    \[\beta^s(x_1^{t-1}) \doteq \frac{P^s_e(x_1^{t-1})}
      {P^{0s}_w(x_1^{t-1})P^{1s}_w(x_1^{t-1})}\]}
\item Interpretation: The ratio between the posterior probability of
  using the KT predictor at $s$ \B{(stop)} and the probability of using the predictions due to the children \B{(continue)}
\item The mixing factors \B{Prior} distribution is \R{$1=0.5/0.5$}
\item If \R{$\beta(s)$} is large: use mostly \R{$P_e^s(X_T=1|x_1^{t-1})$}
\item If \R{$\beta(s)$} is small: use mostly \R{$P^{0s}_w(X_T=1|x_1^{t-1})P^{1s}_w(X_T=1|x_1^{t-1})$}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Outline of algorithm}
\begin{itemize}
\item {\bf Forward Pass}: Traverse the tree from root to leaf.
\item {\bf extend}: Add two children to the leaf. Initialized counts
  to 0,1.
\item {\bf Backward Pass}: Traverse back to root.\\
  For each node
  \R{$s$}
  \begin{itemize}
  \item compute \R{$P_e^s\paren{X_t=1 | x_1^{t-1}}$} and
      \R{$P_w^s\paren{X_t=1 | x_1^{t-1}}$}
    \item update counts: \R{$a^s,b^s$}.
    \item update \R{$\beta^s$}
    \end{itemize}
    \item Complexity: each forward and backwards takes \B{O(depth of tree)}
  \end{itemize}
\end{frame}

\section{Slides from Frans Willems}
%   \newpage
% \bf{Slides from Frans Willems}\\
% \includegraphics[width=4in]{figures/Betas1.pdf}

\newpage
\bf{Slides from Frans Willems}\\
\includegraphics[width=4in]{figures/Betas2.pdf}

% Recaping the algorithm.

% generalizing the algorithm to Hedge.

% Next class thursday.
\iffalse


\fi
\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
