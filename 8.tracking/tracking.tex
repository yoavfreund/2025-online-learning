%\documentclass{beamer}
\documentclass[handout]{beamer}
% This file is a solution template for:

% - Giving a talk on some subject.
% - The talk is between 15min and 45min long.
% - Style is ornate.

% Copyright 2004 by Till Tantau <tantau@users.sourceforge.net>.
%
% In principle, this file can be redistributed and/or modified under
% the terms of the GNU Public License, version 2.
%
% However, this file is supposed to be a template to be modified
% for your own needs. For this reason, if you use this file as a
% template and not specifically distribute it as part of a another
% package/program, I grant the extra permission to freely copy and
% modify this file as you see fit and even to delete this copyright
% notice. 


\mode<presentation>
{
  \usetheme{Montpellier}

  %\setbeamercovered{transparent}
  % or whatever (possibly just delete it)
}

\usepackage{xmpmulti} % package that defines \multiinclude

\usepackage[english]{babel}

\usepackage[latin1]{inputenc}

\usepackage{times}
\usepackage[T1]{fontenc}
% Or whatever. Note that the encoding and the font should match. If T1
% does not look nice, try deleting the line with the fontenc.

\title %[Vovk's algorithm] %(optional, use only with long paper titles)
{Tracking the best Expert}

\author[Freund] % (optional, use only with lots of authors)
{Yoav Freund}
% - Give the names in the same order as the appear in the paper.
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute[Universities of Somewhere and Elsewhere] % (optional, but mostly needed)

\subject{Machine Learning}
% This is only inserted into the PDF information catalog. Can be left
% out. 

% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}



% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
%% \AtBeginSubsection[]
%% {
%%   \begin{frame}<beamer>
%%     \frametitle{Outline}
%%     \tableofcontents[currentsection,currentsubsection]
%%   \end{frame}
%% }


% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 

\beamerdefaultoverlayspecification{<+->}

\input{../macros}

\begin{document}

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
  \titlepage
  Based on ``Tracking the best linear predictor'' and ``Tracking the
  best expert'' by Herbster and Warmuth. Also, section 11.5 in
  Prediction learning and Games.
\end{frame}

\definecolor{mathcolor}{rgb}{1,0,0} % Define red color for math expressions

\begin{small}




\section{Switching Experts}

\begin{frame}
\frametitle{Switching experts setup}
\begin{itemize}
\item \B{Usually:} compare algorithm's total loss to total
  loss of the best expert.
\item \B{Switching experts:} compare algorithm's total loss to total
  loss of \B{best expert sequence} with \R{$k$} \B{switches}.
\item
\includegraphics[width=4.5in]{figures/SwitchingExperts.pdf}
\end{itemize}
\end{frame}

\section{An inefficient algorithm}

\begin{frame}
\frametitle{An inefficient algorithm}
\begin{itemize}
\item Fix:
\begin{itemize}
\item \R{$l$} - sequence length
\item \R{$k$} - number of switches
\item \R{$n$} - number of experts
\end{itemize}
\item Consider one \B{partition-expert} per sequence of switching experts.
\item No. of \B{partition-expert}s : 
\R{${l \choose k-1} n (n-1)^k = O \paren{n^{k+1} \paren{\frac{el}{k}}^k} $}
\item The log-loss regret is at most 
\R{$(k+1) \log n + k \log \frac{l}{k} +k$}
\item Requires maintaining \R{$O \paren{n^{k+1} \paren{\frac{el}{k}}^k}$} weights.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{generalization to mixable losses}
\begin{itemize}
\item In this lecture we assume loss function is \B{mixable}.
\item There is an exponential weights algorithm with learning rate \R{$\eta$} that 
achieves (in the non-switching case) a bound 
\R{\[
L_A \leq \min_i L_i + \frac{1}{\eta} \log n
\]}
\item Then using the \B{partition-expert} algorithm for the switching-experts case 
we get a bound on the regret 
\R{$\frac{1}{\eta} \paren{(k+1) \log n + k \log \frac{l}{k} +k}$}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Weight sharing algorithms}
\begin{itemize}
\item Update weights in two stages: loss update then share update.
\item Prediction uses the normalized \R{$s$} weights \R{$w_{t,i}^s / \sum_j w_{t,j}^s$}
\item \B{Loss update} is the same as always, but defines intermediate \R{$m$} weights:
\R{\[
w_{t,i}^m = w_{t,i}^s e^{-\eta L(y_t,x_{t,i})}
\]}
\item \B{Share update}: redistribute the weights
\item \B{Fixed-share}: 
\R{\begin{eqnarray*}
pool & = & \alpha \sum_{j=1}^n w_{t,j}^m \\
     w_{t+1,i}^s & = & (1-\alpha) w_{t,i}^m
                       + \frac{1}{n-1} \paren{pool - \alpha w_{t,i}^m}
\end{eqnarray*}}
\end{itemize}
\end{frame}

\section{The fixed-share algorithm}

\begin{frame}
\frametitle{The fixed-share algorithm}
\multiinclude[graphics={width=11cm},format=pdf]{figures/IntermediateWeights}
\end{frame}

\begin{frame}
\frametitle{Proving a bound on the fixed-share}
\begin{itemize}
\item The relation between algorithm loss and total weight does not change
because share update does not change the total weight.
\item Thus we still have 
\R{\[
L_A \leq \frac{1}{\eta} \sum_{i=1}^n w_{l+1,i}^s
\]}
\item The harder question is how to lower bound \R{$\sum_{i=1}^n w_{l+1,i}^s$}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Lower bounding the final total weight}
\begin{itemize}
\item Fix some switching experts sequence:
\includegraphics[width=4.5in]{figures/SwitchingExperts.pdf}
\item \B{``follow''} the weight of the chosen expert $i_t$.
\item The loss update reduces the weight by a factor of \R{$e^{-\eta \ell_{t,i_t}}$}.
\item The share update reduces the weight by a factor larger than:
\begin{itemize}
\item \R{$1-\alpha$} on iterations with no switch.
\item \R{$\frac{\alpha}{n-1}$} on iterations where a switch occurs.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Bound for arbitrary $\alpha$}
\begin{itemize}
\item
Combining we lower bound the final weight of the last expert in the sequence
\R{\[
w^s_{l+1,e_k} \geq \frac{1}{n} e^{-\eta L_*} (1-\alpha)^{l-k-1} \paren{\frac{\alpha}{n-1}}^k
\]}
Where \R{$L_*$} is the cumulative loss of the switching sequence of experts.
\item
Combining the upper and lower bounds we get that for any sequence 
\R{\[
L_A \leq L_* + 
\frac{1}{\eta} \paren{ \ln n + \paren{l-k-1} \ln \frac{1}{1-\alpha}
                       +k \paren{ \ln \frac{1}{\alpha} + \ln (n-1)}}
\]}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Tuning $\alpha$}
\begin{itemize}
\item let \R{$k^*$} be the best number of switches (in hind sight) 
and \R{$\alpha^* = k^* / l$}
\item Suppose we use \R{$\alpha \approx \alpha^*$} then the bound that we get is
\R{\[
L_A \leq L_* + \frac{1}{\eta} \paren{(k+1) \ln n 
+ (l-1)\paren{H(\alpha^*) + D_{\text{KL}}(\alpha^* || \alpha)}}
\]}
Where
\R{
\[
H(\alpha^*) = -\alpha^* \ln \alpha^* - (1-\alpha^*) \ln (1-\alpha^*)
\]
\[
D_{\text{KL}}(\alpha^* || \alpha) = 
\alpha^* \ln \frac{\alpha^*}{\alpha}  (1-\alpha^*) \ln \frac{1-\alpha^*}{1-\alpha}
\]
}
\item This is very close to the loss of the computationally inefficient algorithm.
\item For the log loss case this is essentially optimal.
\item Not so for square loss!
\end{itemize}
\end{frame}

\section{The variable-share algorithm}

\begin{frame}
\frametitle{What can we hope to improve?}
\begin{itemize}
\item In the fixed-share algorithm, the 
weight of a suboptimal expert never decreases below
\R{$\alpha/n$}.
\item The regret depends on the length of the sequence.
\item The algorithm does not concentrate only on the best expert, even
if the last switch is in the distant past.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The idea of variable-share}
\begin{itemize}
\item Let the fraction of the total weight given to the 
best expert get arbitrarily close to \R{$1$}.
\item we can get a regret bound that depends only on the number of
switches, not on the lenght of the sequence.
\item Requires that the loss be bounded.
\item Works for \B{square} loss, but not for \B{log} loss!
\end{itemize}
\end{frame}

\begin{frame}
%  \frametitle{Fixed-share vs. Variable-share}
\B{\bf Fixed-share}: 
\R{\begin{eqnarray*}
pool & = & \alpha \sum_{j=1}^n w_{t,j}^m \\
     w_{t+1,i}^s & = & (1-\alpha) w_{t,i}^m
                       + \frac{1}{n-1} \paren{pool - \alpha w_{t,i}^m}
\end{eqnarray*}}

\B{\bf Variable-share}
\R{\begin{eqnarray*}
pool & = & \sum_{i=1}^n \paren{1-\paren{1-\alpha}^{\ell_{t,i}}} w_{t,i}^m \\
w_{t+1,i}^s & = & (1-\alpha)^{\ell_{t,i}} w_{t,i}^m + 
\frac{1}{n-1} \paren{pool - \paren{1-\paren{1-\alpha}^{\ell_{t,i}}} w_{t,i}^m}
\end{eqnarray*}}
\pause
If \R{$\ell_{t,i} = 0$}, then expert \R{$i$} does not contribute to the pool.\\
\pause
If \R{$\ell_{t,i} = 1$}, then expert \R{$i$} contributes like fixed share.\\
\pause
Expert can get fraction of the total weight arbitrarily close to \R{$1$}.\\
\pause
Shares the weight quickly if \R{$\ell_{t,i}>0$}
\end{frame}

\begin{frame}
\frametitle{Bound for variable share}
\R{
\[
L_A - L_* \leq \frac{1}{\eta} \ln n +
\paren{ 1+ \frac{1}{(1-\alpha)\eta}} L_* +
k \paren{1 + \frac{1}{\eta} \paren{ \ln {n-1} + \ln \frac{1}{\alpha} + \ln \frac{1}{1-\alpha} }}
\]
}
\pause
\begin{itemize}
\item \R{$\alpha$} should be tuned so that it is (close to) 
  \R{$\frac{k}{2k+L_*}$}
  \item there is no dependence on $l$ the length of the sequence.
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Some Experiments}
  Setup
  \begin{itemize}
  \item 2-3 expeerts
  \item time is divided into equal length segments
  \item In each segment a different expert is good.
  \end{itemize}
\end{frame}

\begin{frame}
\frametitle{An experiment using static experts}
\includegraphics[height=6cm]{figures/NoShare.png}
\end{frame}

\begin{frame}
\frametitle{An experiment using fixed share}
\includegraphics[height=6cm]{figures/FixedShareFigure.png}
\end{frame}

\begin{frame}
\frametitle{An experiment using variable share}
\includegraphics[height=6cm]{figures/VariableShareFigure.jpg}
\end{frame}


\begin{frame}
  \frametitle{Bounding Regret Using the Pythagorean Inequality}

  \textbf{Pythagorean Inequality:}
  \[
    D_{R}\bigl(w^* \,\|\, w_t\bigr)
    \;\ge\;
    D_{R}\bigl(w^* \,\|\, w_{t+1}\bigr)
    \;+\;
    D_{R}\bigl(w_{t+1} \,\|\, w_t\bigr).
  \]
  \pause
  \textbf{plus three-point identity:}
  \[
    D_{R}(a\,\|\,b) 
    \;+\; 
    D_{R}(b\,\|\,c)
    \;-\; 
    D_{R}(a\,\|\,c)
    \;=\;
    \langle a - b,\;\nabla R(c) - \nabla R(b)\rangle.
  \]
\pause
  \textbf{yields regret bound:}
  \[
    \sum_{t=1}^{T} \langle w_t - w^*,\, z_t\rangle
    \;\le\;
    D_{R}\bigl(w^* \,\|\, w_1\bigr)
    \;+\;
    \sum_{t=1}^{T} D_{R}\bigl(w_{t+1} \,\|\, w_t\bigr).
  \]
  \pause
  \textbf{Interpretation:}
  \begin{itemize}
    \item Switching to a better expert is controlled by 
          \(D_{R}\bigl(w_{t+1}\,\|\,w_t\bigr)\).
    \item The total regret depends on the regularizer \(R(w)\).
  \end{itemize}
\end{frame}

\begin{frame}{Effect on Expert Learning}
\textbf{Standard Mirror Descent:}  
\[
w_{t+1} = \arg\min_{w} \left[ \eta \sum_{s=1}^{t} \langle w, z_s \rangle + D_R(w \| w_1) \right]
\]
\textbf{Regret Bound:}  
\[
\sum_{t=1}^{T} \langle w_t - w^*, z_t \rangle \leq D_R(w^* \| w_1) + \sum_{t=1}^{T} D_R(w_t \| w_{t+1}).
\]
\textcolor{mathcolor}{The Pythagorean inequality controls regret by bounding divergence.}
\end{frame}

\begin{frame}{Fixed Share Algorithm}
\textbf{Fixed Share Update:}  
\[
w_{t+1}^i = (1 - \alpha) \frac{w_t^i e^{-\eta z_t^i}}{\sum_j w_t^j e^{-\eta z_t^j}} + \frac{\alpha}{N}.
\]
\textbf{Impact on Pythagorean Inequality:}
\[
D_R(w^* \| w_t) \geq D_R(w^* \| w_{t+1}) + D_R(w_{t+1} \| w_t).
\]
\textbf{Modification:}  
\textcolor{mathcolor}{The divergence \( D_R(w_{t+1} \| w_t) \) increases due to the uniform mixing factor \( \alpha \).}

\textbf{Regret Bound:}  
\[
\sum_{t=1}^{T} \langle w_t - w^*, z_t \rangle \leq D_R(w^* \| w_1) + \sum_{t=1}^{T} \left[ D_R(w_t \| w_{t+1}) + \alpha D_{KL}(w_t \| u) \right].
\]
\end{frame}

\begin{frame}{Variable Share Algorithm}
\textbf{Variable Share Update:}  
\[
w_{t+1}^i = (1 - \alpha_t) \frac{w_t^i e^{-\eta z_t^i}}{\sum_j w_t^j e^{-\eta z_t^j}} + \alpha_t S_t^i.
\]
\textbf{Impact on Pythagorean Inequality:}
\[
D_R(w^* \| w_t) \geq D_R(w^* \| w_{t+1}) + D_R(w_{t+1} \| w_t) + \alpha_t D_{KL}(w_t \| u).
\]

\textbf{Modification:}  
\textcolor{mathcolor}{The divergence term now depends on \( \alpha_t \), making it adaptive rather than constant.}

\textbf{Regret Bound:}  
\[
\sum_{t=1}^{T} \langle w_t - w^*, z_t \rangle \leq D_R(w^* \| w_1) + \sum_{t=1}^{T} \left[ D_R(w_t \| w_{t+1}) + \alpha_t D_{KL}(w_t \| u) \right].
\]
\end{frame}


\end{small}

\end{document}

\begin{frame}{Comparison: Fixed Share vs. Variable Share}
\begin{table}[]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Algorithm}  & \textbf{Effect on Pythagorean Inequality} & \textbf{Switching Cost} \\ \hline
\textbf{Fixed Share}  & Constant divergence increase & \( \alpha D_{KL}(w_t \| u) \) (fixed) \\ \hline
\textbf{Variable Share} & Adaptive divergence term & \( \alpha_t D_{KL}(w_t \| u) \) (dynamic) \\ \hline
\end{tabular}
\end{table}

\textbf{Key Differences:}
\begin{itemize}
    \item **Fixed Share:** \textcolor{mathcolor}{Adds a fixed switching cost, increasing divergence uniformly}.
    \item **Variable Share:** \textcolor{mathcolor}{Dynamically adjusts the divergence penalty, reducing unnecessary switching}.
\end{itemize}
\end{frame}

\begin{frame}{Summary}
\begin{itemize}
    \item The \textbf{Pythagorean Inequality} ensures **divergence decreases over time**.
    \item **Fixed Share**: \textcolor{mathcolor}{Constant switching cost increases divergence}.
    \item **Variable Share**: \textcolor{mathcolor}{Adaptive switching cost reduces unnecessary updates}.
    \item **Fixed Share is simpler**, but **Variable Share performs better in non-stationary settings**.
\end{itemize}
\end{frame}

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\section{}
\includegraphics[width=4in]{ManfredSlides/skitch9.png}\\
\includegraphics[width=4in]{ManfredSlides/skitch8.png}\\
\includegraphics[width=4in]{ManfredSlides/skitch7.png}\\
\includegraphics[width=4in]{ManfredSlides/skitch6.png}

\begingroup
\small
\begin{frame}
\frametitle{Unnormalized Relative entropy} 
\begin{itemize}
\item prediction, outcome \R{$\vecp,\vecq$} are \R{$n$} dimensional
  vectors with non-negative coordinates. 
\item Loss is RE extended to non-negative vectors:
\R{\[ \RE{\vecp}{\vecq} = \sum_{i=1}^n p_i \log \frac{p_i}{q_i} -
    \sum_{i=1}^n (q_i-p_i) \]}
Coincides with \R{RE} when \R{$\sum_{i=1}^n p_i = \sum_{i=1}^n
  q_i = 1$}
\item Unnormalized \R{RE} is the Bregman divergence corresponding to
  the unnormalized entropy:
  \R{\[
      F(\vecp) = \sum_{i=1}^n p_i \log p_i - \sum_{i=1}^n p_i
    \]}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Inequalities for Unnormalized Relative entropy} 
\begin{itemize}
\item
\B{No} \B{triangle inequality}
\R{$ \exists \vecp_1,\vecp_2,\vecp_3\;\; \RE{\vecp_1}{\vecp_3} >
  \RE{\vecp_1}{\vecp_2} + \RE{\vecp_2}{\vecp_3}
$}
\item \B{Generalized Pythagorean inequality}
  For any closed convex set \R{$S$} and any point \R{$\vecp_1 \notin
    S$}, define the projection of \R{$\vecp_1$} on \R{$S$} to be
  \R{$\vecp_2 = \mbox{argmin}_{\vu \in S} \RE{\vecp_1}{\vu}$}, then:
  \R{\[
       \forall \vecp_3 \in S;\;\; \RE{\vecp_1}{\vecp_3} \geq \RE{\vecp_1}{\vecp_2} + \RE{\vecp_2}{\vecp_3}
\]}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{half squared euclidean distance}
\begin{itemize}
\item prediction, outcome \R{$\vu,\vv \in R^n$}
\item
  \R{\[ \lambda_{\text{sq}}(\vu,\vv)  = \frac{1}{2} \|\vu-\vv\|^2 =
      \frac{1}{2} \sum_{i=1}^n (u_i - v_i)^2
  \]}
\item
Bregman divergence with respect to the square euclidean norm
\R{$$\| \vv \|_2$$}
\item Triangle inequality does not hold.
\item \B{Pythagoras inequality} :
  For any closed convex set \R{$S$} and any point \R{$\vv_1 \notin
    S$}, define the projection of \R{$\vv_1$} on \R{$S$} to be 
  \R{$\vv_2 = \mbox{argmin}_{\vu \in S} \| \vv_1 - \vu \|^2$}, then:
  \R{\[
      \forall \vv_3 \in S;\;\; \| \vv_1 - \vv_3 \|^2 \geq \| \vv_1 - \vv_2 \|^2 + \| \vv_2 - \vv_3 \|^2
    \]}
\end{itemize}
\end{frame}
\endgroup

\includegraphics[width=4in]{ManfredSlides/skitch.2.png}


\begin{frame}
\frametitle{Bregman divergence regularization}
\begin{itemize}
\item Idea: Set $\vw_{t+1}$ to be  \R{$\vu$} that minimizes:
  \R{\[
       \Delta_F(\vw_t,\vu) + \alpha \costvec{t}(\vu) 
  \]}
\item In general, hard to compute the minimum.
\item Efficient approximation \B{Mirror Descent}. Will be covered later.
\end{itemize}
\end{frame}

\includegraphics[width=4in]{ManfredSlides/skitch3.png}\\
\includegraphics[width=4in]{ManfredSlides/skitch.1.png}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
