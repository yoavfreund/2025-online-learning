\documentclass{beamer}
%\documentclass[handout]{beamer}
% This file is a solution template for:

% - Giving a talk on some subject.
% - The talk is between 15min and 45min long.
% - Style is ornate.

% Copyright 2004 by Till Tantau <tantau@users.sourceforge.net>.
%
% In principle, this file can be redistributed and/or modified under
% the terms of the GNU Public License, version 2.
%
% However, this file is supposed to be a template to be modified
% for your own needs. For this reason, if you use this file as a
% template and not specifically distribute it as part of a another
% package/program, I grant the extra permission to freely copy and
% modify this file as you see fit and even to delete this copyright
% notice. 


\mode<presentation>
{
  \usetheme{Montpellier}

  %\setbeamercovered{transparent}
  % or whatever (possibly just delete it)
}

\usepackage{xmpmulti} % package that defines \multiinclude

\usepackage[english]{babel}

\usepackage[latin1]{inputenc}

\usepackage{times}
\usepackage[T1]{fontenc}
% Or whatever. Note that the encoding and the font should match. If T1
% does not look nice, try deleting the line with the fontenc.

\title [Vovk's algorithm] %(optional, use only with long paper titles)
{Vovk's aggregating algorithm \\ Mixable and unmixable loss functions}
\author[Freund] % (optional, use only with lots of authors)
{Yoav Freund}
% - Give the names in the same order as the appear in the paper.
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute[Universities of Somewhere and Elsewhere] % (optional, but mostly needed)

\subject{Machine Learning}
% This is only inserted into the PDF information catalog. Can be left
% out. 

% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}



% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
%% \AtBeginSubsection[]
%% {
%%   \begin{frame}<beamer>
%%     \frametitle{Outline}
%%     \tableofcontents[currentsection,currentsubsection]
%%   \end{frame}
%% }


% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 

\beamerdefaultoverlayspecification{<+->}

\input{../macros}

\begin{document}

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
  \titlepage

  Section 3.5 in ``Prediction, Learning and Games''\\
  Slides in 6.Mixable
\end{frame}

\begingroup
\small

\begin{frame}
  \frametitle{Outline}
  \tableofcontents[pausesections]
  % You might wish to add the option [pausesections]
\end{frame}

\begin{frame}{What we are going to investigate}
\begin{itemize}
\item Different loss functions and their regret bounds.
\item Vovk's Meta Algorithm and it's analysis
\item Instead of associating a loss with an action, we have experts
  that make predictions, and use the following protocol
  \begin{itemize}
  \item Each expert makes a prediction
  \item The algorithm makes a prediction
  \item nature chooses an outcome
  \item each expert, and the algorithm suffer a loss that is a
    function of the prediction and the outcome.
  \end{itemize}
  \item The goal is to minimize the regret.
\end{itemize}
\end{frame}

\section{Log Loss and Absolute loss}

\begin{frame}
\frametitle{Absolute loss}
\begin{itemize}
\item Prediction: \R{$p \in [0,1]$} outcome \R{$x \in \{0,1\}$}
\item Loss: \B{$\lambda(p,x) = |x-p|$}
\item \R{$N$} experts, expert \R{$i$} at time \R{$t$} outputs \R{$q_i^t \in [0,1]$}
\item Cumulative loss of expert \R{$i$} at time \R{$t$}: \R{$L^t_i = \sum_{s=1}^t \lambda(q_i^s,x^s)$}
\item Experts algorithm (Hedge):
  \begin{itemize}
    \item Assign weights: \B{$w_i^t = \frac{1}{N} \exp(-  \eta L_i^{t-1})$}
    \item Master prediction:
    \R{$q_M^t = \frac{\sum_{i=1}^N q_i^t w_i^t}{\sum_{i=1}^N w_i^t}$}
  \end{itemize}
\item Regret Bound for known horizon.
  \begin{itemize}
  \item Set \R{$\eta$} according to \R{$T$}:
    \R{$
      \eta \approx \sqrt{\frac{2 \ln N}{T}}
      $}
  \item Regret bound:
    \R{\[
        L_A \leq \min_i \lossi{i} + \sqrt{2 T \ln N} + \ln N
      \]}
    \item Is there an advantage to the algorithm relative to DTOL?
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Binary log-loss}
\begin{itemize}
\item Prediction: \R{$p \in [0,1]$} outcome \R{$x \in \{0,1\}$}
\item Loss: \R{$\lambda(p,x) = -x \log p - (1-x) \log (1-p)$}
\item \R{$N$} experts, expert \R{$i$} at time \R{$t$} outputs \R{$q_i^t \in [0,1]$}
\item Cumulative loss of expert \R{$i$} at time \R{$t$}: \R{$L^t_i = \sum_{s=1}^t \lambda(q_i^s,x^s)$}
\item Experts algorithm:
  \begin{itemize}
    \item Assign weights: \R{$w_i^t = \frac{1}{N} \exp(-L_i^{t-1})$}
    \item Master prediction:
    \R{$q_M^t = \frac{\sum_{i=1}^N q_i^t w_i^t}{\sum_{i=1}^N w_i^t}$}
  \end{itemize}
\item Regret Bound:
  \R{\[
    L_A^T \leq \min_i L_i^T + \ln N
    \]
  }
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exponential Weights algorithm for log loss = Bayes Algorithm}
\begin{itemize}
\item Prediction: \R{$p \in [0,1]$} outcome \R{$x \in \{0,1\}$}
\item Loss =  \R{$\lambda(p,x) = -x \log p - (1-x) \log (1-p)$}
\item \R{$N$} distribution models, model \R{$i$} at time \R{$t$} assigns probability
  \R{$q_i^t(x_i^t) \in [0,1]$} to outcome \R{$x^s$}
\item Cumulative loss of expert \R{$i$} at time \R{$t$}: \R{$L^t_i = \sum_{s=1}^t \lambda(q_i^s,x^s)$}\\
  Same as minus log likelihood of model \R{$i$}:
  \R{$L^t_i  = -\log \prod_{s=1}^{t-1} q_i^s(x^s)$}
\item algorithm:
  \begin{itemize}
  \item Assign weights to experts: \R{$w_i^t = \frac{1}{N} \exp(-L_i^{t-1})$}
  \item Assign posterior over models= \R{$w_i^t =\frac{1}{N} \prod_{s=1}^{t-1} q_i^s(x_i^s)$}
    \item prediction:
      \R{$q_M^t = \frac{\sum_{i=1}^N q_i^t w_i^t}{\sum_{i=1}^N w_i^t}$}
    \item same as posterior average
  \end{itemize}
\end{itemize}
\end{frame}


\iffalse
\begin{frame}
\frametitle{Two other loss functions over $[0,1]$}
\begin{itemize}
\item Prediction: \R{$p \in [0,1]$} outcome \B{$x \in [0,1]$} (note
  \R{$x$} is not restricted to \R{$\{0,1\}$})
\item \B{Square loss} (Breier Loss): \R{$\lambda(p,x) = (p-x)^2$}
\item \B{Hellinger Loss} \R{$\lambda(p,x)=\frac{1}{2} \paren{
\paren{\sqrt{p} +\sqrt{x}}^2 + 
\paren{\sqrt{1-p}+\sqrt{1-x}}^2 }$}

\end{itemize}
\end{frame}
\fi

\section{The general prediction game}

\begin{frame}
\frametitle{Vovk's general prediction game}
\R{$\Gamma$} - \B{prediction} space.
\R{$\Omega$} - \B{outcome} space. \\
\pause
On each trial \R{$t=1,2,\ldots$}
\pause
\begin{enumerate}
\item
Each expert \R{$i \in \{1 \ldots N \}$} makes a prediction 
\R{$\gamma_i^t \in \Gamma$}
\item
The learner, after observing \R{$\langle \gamma_1^t \ldots \gamma_N^t \rangle$}, \\
makes its own prediction \R{$\gamma^t$}
\item
Nature chooses an outcome \R{$\omega^t \in \Omega$}
\item
Each expert incurs loss \R{$\elloss{i}{t} = \lambda(\omega^t,\gamma_i^t)$} \\
The learner incurs loss \R{$\elloss{A}{t} = \lambda(\omega^t,\gamma^t)$}
\end{enumerate}

\end{frame}

\begin{frame}
\frametitle{Achievable loss bounds}
\begin{itemize}
\item \R{$\TAloss \doteq \sum_{t=1}^T \elloss{A}{t}$} - total loss of algorithm
\item \R{$\TEloss{i} \doteq \sum_{t=1}^T \elloss{i}{t}$} - total loss of expert \R{$i$}
\item \B{\bf Goal:} find an algorithm which guarantees that 
\R{\[
(a,c) \in [0,\infty),\;\; \TAloss \leq a \BEloss + c \ln N 
\]}
For any sequence of events.
\item We say that the pair \R{$(a,c)$} is \B{achievable}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The set of achievable bounds}
\begin{itemize}
\item 
Fix loss function \R{$\lambda: \Omega \times \Gamma \to [0,\infty)$}
\item
The pair \R{$(a,c)$} is {\em achievable} if there exists 
{\em some} prediction algorithm
such that for \B{\em any} \R{$N>0$}, \B{\em any} set of \R{$N$} prediction
sequences and \B{\em any} sequence of outcomes
\R{\[
\TAloss \leq a \BEloss + c \ln N
\]}
\item
\begin{center}
\includegraphics[height=5cm]{figures/achievable1.pdf}
\end{center}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Analysis for specific loss functions}
\begin{itemize}
\item
\B{Outcomes:} \R{$\omega^1,\omega_2,\ldots$ $\omega^t \in [0,1]$}  
\item
\B{Predictions:} \R{$\gamma^1,\gamma^2,\ldots$  $\gamma^t \in [0,1]$}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Log loss (KL-divergence)} 
\begin{itemize}
\item
\R{\[ \lambda_{\text{ent}}(\omega,\gamma) = \omega \ln \frac{\omega}{\gamma} 
                              +(1-\omega) \ln \frac{1-\omega}{1-\gamma} \]}
\item
  If \R{$P[\omega_t=1]=q$}, optimal prediction \R{$\gamma^t=q$}
\item A divergence between (binary) distributions.
\item The ifference between the expected log likelihood and the optimal expected log likelihood
\item
\B{Un}bounded loss.
\item
\B{Not} symmetric \R{$\exists p,q\;\; \lambda(p,q) \neq \lambda(q,p)$}.
\item
\B{No} triangle inequality
\R{$ \exists p_1,p_2,p_3\;\; \lambda(p_1,p_3) > \lambda(p_1,p_2) + \lambda(p_2,p_3)$}
\item    \R{\[
    L_A^T \leq \min_i L_i^T + \ln N
    \]}
\end{itemize}


\end{frame}

\begin{frame}
\frametitle{Square loss (Breier Loss)}
\begin{itemize}
\item
\R{\[ \lambda_{\text{sq}}(\omega,\gamma)  = (\omega-\gamma)^2 \]}
\item
\R{$P[\omega^t=1]=q,\;\; P[\omega^t=0]=1-q$}, \\
optimal prediction \R{$\gamma^t=q$}
\item
Bounded loss.
\item
Defines a metric (symmetric and triangle ineq.)
\item
  Corresponds to regression.
  \item  \R{\[
        L_A^T \leq \min_i L_i^T + \frac{1}{2}\ln N
      \]}

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Hellinger Loss}

\begin{itemize}
\item
\R{\[ \lambda_{\text{hel}}(\omega,\gamma)  = \frac{1}{2} \paren{
\paren{\sqrt{\omega} +\sqrt{\gamma}}^2 + 
\paren{\sqrt{1-\omega}+\sqrt{1-\gamma}}^2 
} \] } 
\item
If \R{$P[\omega^t=1]=q,\;\; P[\omega^t=0]=1-q$}, \\
optimal prediction \R{$\gamma^t=q$}
\item
Loss is bounded.
\item
Defines a metric.
\item 
\R{$\lambda_{\text{hel}}(p,q)  \approx \lambda_{\text{ent}}(p,q)$} when 
\R{$p \approx q$} and \R{$p,q \in (0,1)$}
\item    \R{\[
    L_A^T \leq \min_i L_i^T + \frac{1}{\sqrt{2}}\ln N
\]}

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Absolute loss}

\begin{itemize}
\item
\R{\[ \lambda(\omega,\gamma) = | \omega -\gamma | \]}
\item
Probability of making a mistake if predicting 0 or 1 
using a biased coin\\
\item
If \R{$P[\omega^t=1]=q,\;\; P[\omega^t=0]=1-q$}, then the optimal prediction is 
\R{\[
\gamma^t = 
\begin{cases} 1 & \text{if $q>1/2$,} \\
              0 & \text{otherwise}
\end{cases}
\]}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Structureless bounded loss}

\begin{itemize}
\item Prediction is a distribution 
\R{$\gamma = \langle p_1,\ldots,p_N \rangle$, $p_i \geq 0$, $\sum_{i=1}^N p_i = 1$}
\item Outcome is a loss vector \R{$\omega = \langle \omega_1,\ldots,\omega_N \rangle$, 
$0 \leq \omega_i \leq 1$}
\item Loss is the dot product: \R{$\lambda_{\text{dot}}(\omega,\gamma) = \gamma \cdot \omega$}
\item Corresponds to the hedging game.
\item For hedge loss the regret is \R{$\Omega(\sqrt{T \log N})$}.
\item For the log loss the regret is \R{$O(\log N)$}
\item {\bf Which losses behave like \B{entropy loss} and which behave like \B{hedge loss}}?
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Some technical requirements}
\begin{itemize}
\item There should be a \B{topology} on the prediction set \R{$\Gamma$} such that
\item
\R{$\Gamma$} is compact.
\item
\R{$\forall \omega \in \Omega$}, the function 
\R{$\gamma \to \lambda(\omega,\gamma)$} is \B{continuous}
\item
There is a \B{universally reasonable prediction} \\
\R{$\exists \gamma \in \Gamma$, $\forall \omega \in \Omega$,
$\lambda(\omega,\gamma) < \infty$}
\item
There is \B{no universally optimal prediction} \\
\R{$\neg \exists \gamma \in \Gamma$, $\forall \omega \in \Omega$,
$\lambda(\omega,\gamma) = 0$}
\end{itemize}
\end{frame}

\section{Vovk's algorithm}

\begin{frame}
\frametitle{Vovk's meta-algorithm}
\begin{itemize}
\item Fix an \B{achievable} pair \R{$(a,c)$} and set \R{$\eta=a/c$}
\item \begin{enumerate}
\item
\R{$$
	\weight{i}{t} = {1 \over N} \; e^{-\eta \TEloss{i}^{t}}
$$}
\item
Choose $\gamma_t$ so that, for all $\omega^t \in \Omega$:
\R{\[
\lambda(\omega^t,\gamma^t) - c \ln \sum_i \weight{i}{t} 
\leq
- c \ln \left( \sum_i 
      \weight{i}{t}e^{-\eta \lambda(\omega^t,\gamma_i^t)}
        \right)
\]}
\end{enumerate}
\item
If choice of \R{$\gamma_t$} always exists, then the total loss satisfies:
\R{\[
\sum_t \lambda(\omega^t,\gamma^t)
\leq
- c \ln \sum_i \weight{i}{T+1}
\leq
a \BEloss + c \ln N
\]}
\item
Vovk's result: \B{\em yes!} a good choice for \R{$\gamma_t$} always exists!
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Vovk's algorithm is the the highest achiever {\color{green} [Vovk95]}}

The pair \R{$(a,c)$} is achieved by \B{some} algorithm 
if and only if it is achieved by \B{Vovk's} algorithm.
\pause \\
The separation curve is
\R{$ \left\{ \left. \left( a(\eta),{a(\eta) \over \eta} \right) \right| 
           \eta \in [0,\infty] \right\} $}
\pause
\begin{center}
\includegraphics[height=5cm]{figures/achievable2.pdf}
\end{center}
\end{frame}

\section{mixable loss functions}
\begin{frame}
\frametitle{Mixable Loss Functions}
\begin{itemize}
\item A Loss function is \B{mixable} if a pair of the form \R{$(1,c),\; c<\infty$} is achievable.
\R{\[
\TAloss \leq \BEloss + c \ln N 
\]}
\item Vovk's algorithm with \R{$\eta = 1/c$} achieves this bound.
\item \R{$\lambda_{\text{ent}},\lambda_{\text{sq}},\lambda_{\text{hel}}$} are \B{mixable}
\item \R{$\lambda_{\text{abs}},\lambda_{\text{dot}}$} are \B{not mixable}
\end{itemize}
\end{frame}


\section{The convexity condition}

\begin{frame}
\frametitle{The convexity condition}
\begin{itemize}
\item requirement for loss to be \R{$(1,1/\eta)$} mixable
\item 
\R{$\forall \langle (\gamma_1,\weight{1}{}),\ldots,(\gamma_N,\weight{N}{}) \rangle$} \\
\R{$\exists \B{\gamma} \in \Gamma$} \\
\R{$\forall \omega \in \Omega$}:
\R{\[
\lambda(\omega,\B{\gamma}) - \frac{1}{\eta} \ln \sum_i \weight{i}{} 
\leq
- \frac{1}{\eta} \ln \left( \sum_i 
      \weight{i}{}e^{-\eta \lambda(\omega,\gamma_i)}
        \right)
\]}
\item
Can be re-written as:
\R{\[
e^{-\eta \lambda(\omega,\B{\gamma})}
\geq
\sum_i 
\left({\weight{i}{} \over \sum_j \weight{j}{}}\right)
e^{-\eta \lambda(\omega,\gamma_i)}
\]}
%% \item \B{Assumption:}
%% fix \R{$\lambda(\omega,\gamma_i)$} for all but \R{$i \not\in \{j,k\}$}
%% then increasing \R{$\lambda(\omega,\gamma_j)$} decreases
%% \R{$\lambda(\omega,\gamma_k)$}
\item
Equivalently - the image of the set \R{$\Gamma$} under the
mapping
\R{$F(\gamma) = 
\left\langle 
   e^{-\eta \lambda(\omega,\gamma)} 
\right\rangle_{\omega \in \Omega}
$} is concave.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{convexity condition: Pictorially}
\begin{itemize}
\item
\noindent {\bf Example:} Suppose $\Omega=\{0,1\}$, $\Gamma = [0,1]$.
then
\[
F(\gamma) = \left\langle
  e^{-\eta \lambda(0,\gamma)},e^{-\eta \lambda(1,\gamma)}
\right\rangle
\]
\item
\begin{center}
\includegraphics[height=5cm]{figures/convex.pdf}
\end{center}
\end{itemize}
\end{frame}

\section{Log loss}

\begin{frame}
\frametitle{Vovk Algorithm for log loss}
\begin{itemize}
\item The log loss is mixable with \R{$\eta=1$}
\item The image of \R{$[0,1]$} through \R{$F(\gamma) = \left\langle
  e^{-\eta \lambda(0,\gamma)},e^{-\eta \lambda(1,\gamma)} \right\rangle$} is a straight line segment.
\item The \B{only} satisfactory prediction is 
\R{\[
\gamma = \frac{\sum_i \weight{i}{} \gamma_i}{\sum_i \weight{i}{}}
\]}
\item
We are back to the online Bayes algorithm.
\end{itemize}
\end{frame}

\section{Square loss}

\begin{frame}
\frametitle{Vovk algorithm for square loss}
\begin{itemize}
\item The square loss is mixable with $\eta=2$.
\item Prediction must satisfy
\R{\[
1-\sqrt{ -{1 \over 2} \ln \sum_i \Nweight{i}{t} e^{-2(1-p^t_i)^2} }
\leq
p^t
\leq
\sqrt{ -{1 \over 2} \ln \sum_i \Nweight{i}{t} e^{-2(p^t_i)^2} }
\]}
where
\R{$
	\Nweight{i}{t} = {\weight{i}{t} \over \sum_s \weight{i}{s}}~.
$}
\item
\R{$$ \TAloss \leq \BEloss + \frac{1}{2}\ln N $$}
\end{itemize}
\end{frame}

\subsection{Square loss using simple averaging}

\begin{frame}
\frametitle{Simple prediction for square loss}
\begin{itemize}
\item We can use the prediction
\R{\[
\gamma = \frac{\sum_i \weight{i}{} \gamma_i}{\sum_i \weight{i}{}}
\]}
\item But in that case we must use \R{$\eta=1/2$} when updating the weights.
\item Which yields the bound
\R{$$ \TAloss \leq \BEloss + 2 \ln N $$}
\end{itemize}
\end{frame}

\section{Summary table}

\begin{frame}
\frametitle{Summary of bounds for mixable losses}
\includegraphics[height=6cm]{figures/summarytable.jpg}
\end{frame}

\endgroup
\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
